\subsection{Linear Least Squares}\label{ed56ce5}

\Problem{Least squares problem}\label{c5a0d84}

In real-life situations, we take lots of experimental data and try to fit the
best model to it. This means that we will have to deal with overdetermined
systems. Consider a matrix $A\in\R^{m\times n}$ such that $m\geq n$. Then
$Ax=b$ likely has no solution. In this case, we still seek to make the best of
our experimental results and try for
$$
  \min_{x\in\R^n}\norm{Ax-b}_2
$$

which minimizes the squared distance between each value of $Ax$ and the
corresponding value in $b$.

\Definition{Normal equations}\label{b17beaa}

Let $A\in\R^{m\times n}$ and $b\in\R^m$. The normal equations are
$$
  A^TA\hat x=A^Tb
$$

\Definition{Pseudoinverse matrix}\label{f8744da}

Let $A$ be a full rank matrix and consider the \href{b17beaa}{normal
equations}. Since $A$ is full rank, $A^TA$ is positive definite (by
\href{fd1f53e}{this}) and invertible (by \href{d4f72eb}{this}). Moreover, since
$A^TA$ is invertible the solution can be expressed as
$$
  \hat x=(A^TA)^{-1}A^Tb
$$

Note the parallels between solving the system $Mx=b$. If $M$ is invertible,
then $x=M^{-1}b$. Hence, we call $(A^TA)^{-1}A^T$ the \textbf{pseudo-inverse}
of $A$, denoted by $A^+$.

Note that if in addition $A$ is also overdetermined, then we have $A^+A=I$ but
$AA^+\neq I$.

\Lemma{Solve normal equations → solve least-squares}\label{c10737c}

Let $A\in\R^{m\times n}$ be a full rank matrix, and $b\in\R^m$. Consider the
least squares problem
$$
  \min_{x\in\R^n}\norm{Ax-b}_2
$$

If we can find a solution $\hat x$ to the \href{b17beaa}{normal equations}
$$
  A^TAx=A^Tb,
$$

then $\hat x$ minimizes least squares (i.e. solves the minimization problem
above).

\begin{proof}
  Minimizing $\norm{Ax-b}_2$ is the same as minimizing $\norm{Ax-b}_2^2$, which
  can be expanded to
  \begin{align*}
    \norm{Ax-b}_2^2 &=(Ax-b)^T(Ax-b)                  \\
                    &=(Ax)^T(Ax)-b^T(Ax)-(Ax)^Tb+b^Tb \\
                    &=(Ax)^T(Ax)-2(Ax)^Tb+b^Tb        \\
                    &=x^TA^TAx-2x^TA^Tb+b^Tb\Tag{*}
  \end{align*}

  This is a multivariable quadratic polynomial. These have a general form of
  $$
    p(x)=x^TQx-2x^Tr+s
  $$

  Now suppose that $Q$ is symmetric \href{e25e722}{positive semi-definite}, and
  that there is some $y$ such that $Qy=r$. Then
  \begin{align*}
    p(x) &=x^TQx-2x^Tr+s                   \\
         &=x^TQx-2x^TQy+(y^TQy-y^TQy)+s    \\
         &=(x-y)^TQ(x-y)+[s-y^TQy]\Tag{**}
  \end{align*}

  Since $Q$ is positive semi-definite, $(x-y)^TQ(x-y)\geq0$. Notice that
  between the two terms at $(**)$, this is the only term affected by the input
  $x$. Hence, minimizing this term minimizes $p(x)$. Clearly, if this term is
  zero then $p(x)$ is minimized.

  Now if we choose $x:=y$, then $(x-y)^TQ(x-y)=0$ and hence $p(x)$ is
  minimized.

  Applying this to the least-squares problem, using $Q:=A^TA$ and $r:=A^Tb$,
  observe that $Q=A^TA$ is \href{a1d41cd}{symmetric} and also
  \href{fd1f53e}{positive semi-definite} if $A$ is full rank. Hence from $(*)$,
  we obtain the least-squares solution by choosing $x$ such that $x$ solves
  $Qx=r$, or simply $A^TAx=A^Tb$.
\end{proof}

\Lemma{Normal equations have a solution}\label{e1ea52d}

Let $A\in\R^{m\times n}$ and $b\in\R^m$. Then the \href{b17beaa}{normal
equations}
$$
  A^TAx=A^Tb
$$

has at least one solution.

\begin{proof}
  By \href{a1227c1}{this result}, $\range A^T=\range A^TA$. Therefore, for all
  $y=A^Tb$, $y\in\range A^T$ and hence $y\in\range A^TA$, which implies that
  there exists $x\in\R^n$ such that $A^TAx=y$.
\end{proof}

\Lemma{$A$ full rank → normal equations have a unique solution}\label{de838fa}

Let $A\in\R^{m\times n}$ be a full rank matrix, and $b\in\R^m$. Then the
\href{b17beaa}{normal equations}
$$
  A^TAx=A^Tb
$$

have a unique solution.

\begin{proof}
  By \autoref{e1ea52d}, there's at least one solution to the normal equations.

  Since $A$ is full rank, \href{d4f72eb}{$A^TA$ is invertible}. So now let both
  $\hat x$ and $\hat y$ be solutions to the normal equations. Then
  $$
    A^TA\hat x=A^Tb\implies\hat x=(A^TA)^{-1}A^Tb
  $$

  and
  $$
    A^TA\hat y=A^Tb\implies\hat y=(A^TA)^{-1}A^Tb=\hat x
  $$

  hence the solution is unqiue.
\end{proof}

\Theorem{$A$ full rank → least squares has unique solution}\label{ad500c9}

Let $A\in\R^{m\times n}$ be a full rank matrix, and $b\in\R^m$. Then the
\href{b17beaa}{normal equations} have a unique solution that solves the
\href{c5a0d84}{least squares problem}
$$
  \min_{x\in\R^n}\norm{Ax-b}_2
$$

\begin{proof}
  By \autoref{e1ea52d} and \autoref{de838fa}, the normal equations have a unique
  solution. By \autoref{c10737c}, this solves the least squares problem.
\end{proof}

\Theorem{$A$ full rank → least squares has unique solution*}\label{d2262b9}

In \href{c5a0d84}{least squares problem}
$$
  \min_{x\in\R^n}\norm{Ax-b}_2,
$$

If $A\in\R^{m\times n}$ has full column rank, then there is a unique solution
$\bar x$. Moreover, $\bar x$ satisfies the \href{b17beaa}{normal equations}
$$
  A^TAx=A^Tb
$$

This is effectively the same claim as \autoref{ad500c9}. Only the proof is
different.

\begin{proof}
  \def\dx{\Delta x}
  Let $\psi:\R^n\to\R$ be given by
  \begin{align*}
    \psi(x) &:=\frac12\norm{Ax-b}^2                                        \\
            &=\frac12\sum_{i=1}^m\biggl(b_i-\sum_{j=1}^na_{ij}x_j\biggr)^2
  \end{align*}

  Let $\bar x\in\R^n$ such that $\bar x$ minimizes $\psi$. Then by the
  \href{dc165c9}{first-order necessary optimality condition}, we have
  \begin{equation*}
    \frac{\partial}{\partial x_k}\psi(\bar x)
    =\sum_{i=1}^m\biggl[\biggl(b_i-\sum_{j=1}^na_{ij}\bar x_j\biggr)(-a_{ik})\biggr]=0
  \end{equation*}

  for each $k=\iter1n$. Then latter expression can be rewritten as
  $$
    \sum_{i=1}^ma_{ik}\sum_{j=1}^na_{ij}\bar x_j=\sum_{i=1}^ma_{ik}b_i,\with{\forall k=\iter1n}
  $$

  In matrix-vector form this expression looks much simpler:
  $$
    A^TA\bar x=A^Tb
  $$

  Note that we can also express $\psi$ as
  \begin{align*}
    \psi(x) &:=\frac12\norm{Ax-b}^2                    \\
            &=\frac12(b-Ax)^T(b-Ax)                    \\
            &=\frac12x^TA^TAx-b^T(Ax)+\frac12\norm b^2
  \end{align*}

  Then clearly $\psi$ is twice continuously differentiable, with
  \begin{equation*}
    \nabla\psi(x)=A^TAx-b^TA\Quad\text{and}\Quad\nabla^2\psi(x)=A^TA.
  \end{equation*}

  Also, note that since $A$ is full rank, its null space only has the zero
  vector, and we can show that $A^TA$ is positive definite by having any
  $u\in\R^n\sans0$ and seeing that
  \begin{align*}
    u^TA^TAu=(Au)^T(Au)=\norm{Au}^2>0
  \end{align*}

  By the \href{b43d95d}{2nd-order sufficient optimality condition}, $\bar x$ is
  a strict local minimizer of $\psi$.

  But since \href{de25005}{quadratic functions are convex}, by a
  \href{f546fc9}{known result} we have that $\bar x$ is the strict global
  minimizer of $\psi$.
\end{proof}

\Remark{Applying least squares}\label{bbde001}

Consider the situation where we have to fit a model to the data points
$(-2,4)$, $(-1,1)$, $(1,1)$, $(2,3)$. Then we can try a quadratic model with
$$
  y=ax^2+bx+c
$$

where our goal is to find the best triple $(a,b,c)$ to obtain a least-squares
solution.

Using the data points, we obtain
$$
  \begin{pmat}
    4 & -2 & 1 \\
    1 & -1 & 1 \\
    1 & 1  & 1 \\
    4 & 2  & 1 \\
  \end{pmat}
  \begin{pmat}
    a \\b\\c\\
  \end{pmat}
  =\begin{pmat}
    4 \\1\\1\\3 \\
  \end{pmat}
$$

For example, at the point $(4,-2)$ we have $(-2)^2a+(-2)b+c=4$, which
corresponds to the first row.

Now this maps directly to the \href{c5a0d84}{least squares problem}, and we can
go ahead and find the optimum triple.
