\subsection{Definitions}\label{d165fe8}

\Definition{Softmax function}\label{fdaba2f}

Often used as an activation function, the \textit{softmax} function
$\sigma:\R^n\to\R^n$ is given by
$$
  \sigma(\mathbf z)_i:=\frac{e^{z_i}}{\sum_{j=1}^ne^{z_j}}
$$

\Definition{Rectified linear unit}\label{fe694c2}

Often used as an activation function, the \textit{ReLU} function $f:\R\to\R$ is
given by
$$
  x\mapsto\begin{cases}x,&\text{if }x>0\\0,&\text{otherwise}\end{cases}
$$
