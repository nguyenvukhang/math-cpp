\subsection{Abstract}\label{a8a8f03}

\Remark{The abstract}\label{a216d4b}

The following abstract is taken almost verbatim from the original paper.

The purpose of this paper is to investigate neural network capability
systematically. The main results are:
\begin{itemize}
  \item Every \href{e65fc0e}{Tauber-Wiener} function is qualified as an
        activation function in the hidden layer of a three-layered neural
        network.
  \item For a continuous function in $\mathcal S'(\R^1)$ to be a Tauber-Wiener
        function, the necessary and sufficient condition is that it is not a
        polynomial.
  \item The capability of approximating nonlinear functionals defined on some
        compact set of a Banach space and nonlinear operators has been shown,
        which implies that
  \item We show the possibility by neural computation to approximate the output
        as a whole (not at a fixed point) of a dynamical system, thus
        identifying the system.
\end{itemize}

\Remark{Criteria for activation functions}\label{d4a14c6}

Hornik [6] proved that any bounded nonconstant continuous function is qualified
to be an activation function. Mhaskar and Micchelli [11] showed that under some
restriction on the amplitude of a continuous function near infinity, any
non-polynomial function is qualified to be an activation function.
