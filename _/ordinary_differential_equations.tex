\chapter{Ordinary Differential Equations}\label{e598fa6}

\subsection{First order differential equations}\label{e55966b}

\Result{Separable}\label{c4db619}

$$
  \frac{dy}{dx}=\frac{p(x)}{q(y)}
$$

To solve, we do
$$
  \int p(x)\,dx=\int q(y)\,dy
$$

\Result{Homogeneous}\label{bb1ee90}

$$
  \frac{dy}{dx}=g\left(\frac yx\right)
$$

An easier way to check if $\displaystyle\frac{dy}{dx}$ satisfies this form is
by letting $f(x,y):=\displaystyle\frac{dy}{dx}$ and verifying that
$f(x,y)=f(kx,ky)$.

To solve, let $u:=y/x$ and rewrite the DE in terms of $u$ and $x$.

\Result{Linear}\label{b8c7e19}

$$
  \frac{dy}{dx}+p(x)y=q(x)
$$

To solve, let $\ln u:=\int p(x)\,dx$ and jump to
$$
  \frac d{dx}uy=u\cdot q(x)
$$

(which is just the product rule of differentiation)

\Result{Bernoulli}\label{dc54d9b}

$$
  \frac{dy}{dx}+p(x)y=q(x)\cdot y^n
$$

If $n\in\{0,1\}$, we have the \href{b8c7e19}{linear case}.

Use $u:=y^{1-n}$, eliminate all $y$s, and reduce to a linear DE in $u$:
$$
  \frac{du}{dx}+(1-n)p(x)u=(1-n)q(x)
$$

\Result{Riccati}\label{e5b78d0}

$$
  \frac{dy}{dx}=p(x)y^2+q(x)y+r(x)
$$

To solve, first find a basic solution $y_1$. (By inspection, hopefully. Usually
a solution will be given as part of the homework problem)

Then let $\displaystyle y_2:=y_1+\frac1u$, substitute it into the original DE,
and reduce it to a linear DE in $u$:
$$
  -u'=(2py_1+q)u+p
$$

\begin{compute}
  First we obtain $\displaystyle y_2'=y_1'-\frac{u'}{u^2}$. Then substitute into the original DE:
  \begin{align*}
    y_2' &= py_2^2+qy_2+r                                                                  \\
    y_1'-\frac{u'}{u^2}
         &= p\left(y_1+\frac1u\right)^2+q\left(y_1+\frac1u\right)+r                        \\
         &= p\left(y_1^2+\frac{2y_1}{u}+\frac1{u^2}\right)+q\left(y_1+\frac1u\right)+r     \\
         &= (py_1^2+qy_1+r)+p\left(\frac{2y_1}{u}+\frac1{u^2}\right)+q\left(\frac1u\right)
  \end{align*}

  Since $y_1$ is a solution to the original DE,
  \begin{align*}
    -\frac{u'}{u^2} &= p\left(\frac{2y_1}{u}+\frac1{u^2}\right)+q\left(\frac1u\right) \\
    -u'             &=(2py_1+q)u+p
  \end{align*}
\end{compute}

\Result{Exact}\label{db2397c}

$$
  M(x,y)+N(x,y)\frac{dy}{dx}=0
$$

Criteria: $M_y=N_x$.

The idea is to work towards a function $F$ where $F_x=M$, and $F_y=N$. (Because
then based on the original DE we'll have $df/dx=0$)

To solve, integrate to find $F(x,y)=\int M(x,y)\,dx=\int N(x,y)\,dy$.

Then we have $F(x,y)=C$ for some constant $C$.

\subsection{Second order differential equations}\label{b07a82f}

\begin{itemize}
  \item $ay''+by'+cy=g(t)$ : \href{af8932c}{Method of undetermined
        coefficients}
  \item $ax^2y''+bxy'+cy=0$ : \href{c15a777}{Euler equations}
  \item $y''+p(x)y'+q(x)y=r(x)$ : Variation of parameters for either a
        \href{cc51fcb}{particular} or
        \href{d359b97}{complementary} solution.
\end{itemize}

\Definition{Wronskian}\label{b70073b}

The Wronskian of two differentiable functions $f$ and $g$ is $W(f,g):=fg'-gf'$.

More generally, for $n$ complex-valued functions $f_1,\ldots,f_n$ which are
$n-1$ times differentiable on an interval $I$, the Wronskian
$W(f_1,\ldots,f_n)$ is a function on $x\in I$ defined by
$$
  W(f_1,\ldots,f_n)(x):=\det\begin{bmat}
    f_1(x)         & f_2(x)         & \ldots & f_n(x)         \\
    f_1'(x)        & f_2'(x)        & \ldots & f_n'(x)        \\
    \vdots         & \vdots         & \ddots & \vdots         \\
    f_1^{(n-1)}(x) & f_2^{(n-1)}(x) & \ldots & f_n^{(n-1)}(x) \\
  \end{bmat}
$$

\Result{Method of undetermined coefficients}\label{af8932c}

$$
  ay''+by'+cy=g(t)
$$

Define $\Sigma_{n,t}(A):=A_0t^n+A_1t^{n-1}+\ldots+A_n$ and
$P_n(t):=a_0t^n+a_1t^{n-1}+\ldots+a_n$.

\begin{center}
  \def\squirl{\begin{cases}\sin\beta t\\\cos\beta t\end{cases}}
  \def\S#1{\Sigma_{n,t}({#1})}
  \begin{tabular}{|c|c|}
    \hline
    $g(t)$                      & $y(t)$                                                                   \\
    \hline
    $P_n(t)$                    & $\S A$                                                                   \\
    \hline
    $P_n(t)e^{\alpha t}$        & $t^se^{\alpha t}\cdot\S A$                                               \\
    \hline
    $P_n(t)e^{\alpha t}\squirl$ & $t^s\bigl[\S Ae^{\alpha t}\cos\beta t+\S Be^{\alpha t}\sin\beta t\bigr]$ \\
    \hline
  \end{tabular}
\end{center}

Here, $s$ is the smallest non-negative integer that ensures that no term in
$y(t)$ is a solution of the corresponding homogeneous equation.

\Result{Variation of parameters (particular: $y_p$)}\label{cc51fcb}

import \href{b70073b}{Wronskian} for $W$.
$$
  y''+p(x)y'+q(x)y=r(x)
$$

Suppose we already know that $y_1$ and $y_2$ satisfy the corresponding
homogeneous equation.

Then we can jump to
$$
  y_p=v_1y_1+v_2y_2
$$

where $\displaystyle v'_1:=\frac{-y_2r}{W(y_1,y_2)}$ and $\displaystyle
v'_2:=\frac{y_1r}{W(y_1,y_2)}$.

\Result{Variation of parameters (complementary: $y_c$)}\label{d359b97}

$$
  y''+p(x)y'+q(x)y=r(x)
$$

Suppose we already know that $y_1$ is a solution. Then let
$$
  y_2:=vy_1
$$

Then substituting $y_2$ back into the original DE, we have
$$
  v''y_1+v'(2y_1'+py_1)=0
$$

\begin{compute}
  $$
    y_2:=vy_1;\quad y_2'=v'y_1+vy_1';\quad y_2''=v''y_1+2v'y_1'+vy_1''
  $$

  Substituting this back into the original DE:
  $$
    (v''y_1+2v'y_1'+vy_1'')+p(v'y_1+vy_1')+q(vy_1)=r
  $$

  But since $y_1$ is known to be a solution:
  \begin{align*}
    (v''y_1+2v'y_1')+p(v'y_1) &=0 \\
    v''y_1+v'(2y_1'+py_1)     &=0
  \end{align*}
\end{compute}

Which is a first-order linear equation in $v'$. Use $u:=v'$ to solve for $u$
and then substitute everything back to find $y$.

\Result{Euler equations}\label{c15a777}

$$
  ax^2y''+bxy'+cy=0
$$

Try $y=x^r$ for some $r\in\C$ to be found.

If two distinct roots: $y:=Ax^{r_1}+Bx^{r_2}$.

If one distinct root: $y:=Ax^r+B\ln(x)x^r$.

If complex roots ($r=\alpha\pm\beta i$): $y:=Ax^\alpha\cos(\beta\ln
x)+Bx^\alpha\sin(\beta\ln x)$

\subsection{Higher order differential equations}\label{cca5f46}

% Lecture 12 page 6

\begin{equation*}
  \def\y#1{y^{(#1)}}
  \y n+p_1\y{n-1}+p_2\y{n-2}+\ldots+p_{n-1}y'+p_ n y=q
\end{equation*}

where $\iter{p_1}{p_n},q:\R\to\R$.

For these we split into a few cases:
\begin{itemize}
  \item\href{cd8a29a}{Constant coefficients}
  \item\href{a0f8e0c}{Euler equations}
  \item\href{de785b7}{Variation of parameters}
\end{itemize}

\Result{Constant coefficients}\label{cd8a29a}

% Lecture 13 page 2

\begin{equation*}
  \def\y#1{y^{(#1)}}
  \y n+a_1\y{n-1}+a_2\y{n-2}+\ldots+a_{n-1}y'+a_ny=0\Tag{*}
\end{equation*}

where $\iter{a_1}{a_n}\in\R$ fixed.

Then try $y=e^{rt}$. Substituting that into $(*)$, we'll obtain
$$
  a_0r^n+a_1r^{n-1}+\ldots+a_n=0
$$

Since $\iter{a_1}{a_n}$ are given, we can solve for $r$.

If all the roots of $r$ are real and no two are equal, then we have $n$
distinct solutions $\iter{e^{r_1t}}{e^{r_nt}}$ of equation $(*)$.

Moreover, if these solutions are linearly independent, then the general
solution to $(*)$ is
$$
  y_g=c_1e^{r_1t}+c_2e^{r_2t}+\ldots+c_ne^{r_nt}
$$

One way to establish the linear independence is to evaluate their
\href{b70073b}{Wronskian}.

\Result{Euler equations}\label{a0f8e0c}

\begin{equation*}
  \def\y#1{x^{#1}y^{(#1)}}
  a_0\y{n}+a_1\y{n-1}+\ldots+a_ny=0\Tag{*}
\end{equation*}

where $\iter{a_1}{a_n}\in\R$ fixed.

Then try $y=x^{r}$. Substituting that into $(*)$, we'll obtain
$$
  \sum_{i=0}^n\left(\prod_{j=r-i+1}^rj\right) a_ix^r=0
$$

Which is then
$$
  \sum_{i=0}^n\left(\prod_{j=r-i+1}^rj\right) a_i=0
$$

\Result{Variation of parameters}\label{de785b7}

% to add

\Result{Radius of convergence of power series}\label{ea9daf1}

Consider the power series
$$
  \sum_{k=0}^n(ax)^k=1+ax+a^2x^2+a^3x^3+\ldots
$$

As $n\to\infty$,
$$
  \sum_{k=0}^n(ax)^k\begin{cases}
    \text{is convergent on }\dfrac1{1-ax} & \text{if }|ax|<1 \\
    \text{is divergent}                   & \text{otherwise}
  \end{cases}
$$

We use this as a benchmark to check for convergence of an arbitrary power
series:
$$
  \sum_{k=0}^\infty b_kx^k
$$

where $b_1,b_2,\ldots\in\R$.

If for all $k\in\N_0$ we have $|b_k|<a^k$, and the geometric series converges
for $x$, then the arbitrary series converges too.
% prove why |bₖ|<aᵏ implies convergence of the arbitrary series

This is the same as requiring $(b_k)^{1/k}<a$ and $|x|<\frac1{|a|}$, and hence
we obtain the radius of convergence of the arbitrary power series:
$$
  R:=\frac{1}{\displaystyle\lim_{k\to\infty}(b_k)^{1/k}}
$$

The series converges if $|x|<R$, and diverges if $|x|>R$.

\Proposition{Power series at zero}\label{fe2e0f1}

Suppose we have an arbitrary power series that is zero
$$
  \sum_{k=0}^na_kx^k = 0
$$

Then we must have
$$
  a_k=0\quad\forall(k\in\N_0)
$$

\Definition{Laplace transform}\label{a7047c9}

The Laplace transform of a function $f(t)$, defined for all $t\in\R$, $t>0$, is
the function $F(s)$, a unilateral transform defined by
$$
  F(s):=\int_0^\infty f(t)e^{-st}\,dt
$$

Here's a few fundamental Laplace transforms:
\begin{center}
  \def\L#1{\mathcal L\{{#1}\}(s)}
  \def\a{\alpha}\def\b{\beta}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{|c|c|}
    \hline
    $f(t)$                   & $F(s)$                       \\\hline
    $1$                      & $\frac1s$                    \\\hline
    $t$                      & $\frac1{s^2}$                \\\hline
    $t^n$                    & $\frac{n!}{s^{n+1}}$         \\\hline
    $e^{\a t}$               & $\frac1{s-\a}$               \\\hline
    $te^{\a t}$              & $\frac1{(s-\a)^2}$           \\\hline
    $\cos(\b t)$             & $\frac{s}{s^2-\b^2}$         \\\hline
    $\sin(\b t)$             & $\frac{\b}{s^2-\b^2}$        \\\hline
    $e^{\alpha t}\cos(\b t)$ & $\frac{s-\a}{(s-\a)^2+\b^2}$ \\\hline
    $e^{\alpha t}\sin(\b t)$ & $\frac{\b}{(s-\a)^2+\b^2}$   \\\hline
  \end{tabular}
\end{center}

Let $\mathcal L(f):=\mathcal L\{f\}(s)$. Then we have
$$
  \mathcal L(f')=-f(0)+s\mathcal L(f)
$$

\begin{compute}
  Let's work through a few examples together.

  Let $f(t):=1$. Then
  $$
    \mathcal L\{f\}(s)
    =\int_0^\infty e^{-st}\,dt
    =\left[-\frac1se^{-st}\right]_0^\infty
    =\frac1s
  $$

  Next, let $f(t):=t$. Then
  \begin{align*}
    \mathcal L\{f\}(s)
     &=\int_0^\infty te^{-st}\,dt                                                    \\
     &=\left[-\frac1se^{-st}\cdot t\right]_0^\infty-\int_0^\infty-\frac1se^{-st}\,dt \\
     &=\frac1{s^2}
  \end{align*}
\end{compute}

\Exercise{}\label{b0e53e0}

Solve the differential equation
$$
  y'' + 2ty' - 4y = 5\Quad y(0)=9,y'(0)=0
$$

\begin{compute}
  \def\L{\mathcal L}

  Let $u(s):=\L(y)$

  Let's start by preprocessing some Laplace transforms:
  \begin{alignat*}{2}
    \L(y')  & =-f(0)+su          &  & = 9+su     \\
    \L(y'') & =-f'(0)-sf(0)+s^2u &  & = -9s+s^2u
  \end{alignat*}

  And then,
  $$
    \L(ty')=-\frac{d}{ds}\L(y')=-u-su'
  $$

  So then applying the Laplace transform on both sides:
  \begin{align*}
    (-9s+s^2u)+2(-u-su')-4(u)         &=5                          \\
    (-2s)u'+(s^2-6)u                  &=9s+5                       \\
    u'+\frac{6-s^2}{2s}u              &=-\frac92-\frac5{2s}        \\
    u'+\left(\frac3s-\frac s2\right)u &=-\frac92-\frac5{2s}\Tag{*}
  \end{align*}

  This is a first-order linear equation in $u$.

  Let $\displaystyle\ln v:=\int\left(\frac3s-\frac s2\right)\,ds$
  $$
    \ln v = 3\ln s-\frac{s^2}4
    = \ln s^3-\ln e^{(s^2/4)}
    = \ln (s^3e^{-(s^2/4)})
  $$

  $\implies v=s^3e^{-(s^2/4)}$. Substituting back to $(*)$, we have
  \begin{align*}
    uv &=\int\frac{9s+5}{-2s}\cdot s^3e^{-(s^2/4)}\,ds           \\
       &=\int\left(-\frac92s^2-\frac5{2}s\right)e^{-(s^2/4)}\,ds
  \end{align*}

  \begin{align*}
    \int xe^{-(x^2/4)}\,dx   &= -2e^{-(x^2/4)}                           \\
    \int x^2e^{-(x^2/4)}\,dx &= x(-2e^{-(x^2/4)})-\int-2e^{-(x^2/4)}\,dx \\
                             &= x(-2e^{-(x^2/4)})+\int2e^{-(x^2/4)}\,dx
  \end{align*}
\end{compute}

\Definition{Regular singular points}\label{f490530}

This is prerequisite knowledge for the next section on \href{fdab69a}{series
solutions}.

Consider the following ODE:
\begin{equation*}
  y''+P(x)y'+Q(x)y=R(x)\Tag{*}
\end{equation*}

If we have both limits of
\begin{equation*}
  \lim_{x\to x_0}(x-x_0)P(x)\quad\text{and}\quad\lim_{x\to x_0}(x-x_0)^2Q(x)
\end{equation*}

finite, then $x_0$ is a regular singular point of the equation $(*)$.

\Remark{Series solutions}\label{fdab69a}

The main idea here is to try to write $y$ as a power series:
$$
  y:=\sum_{n=0}^\infty a_n(x-x_0)^n
$$

for some fixed $x_0,a_0,a_1,\ldots\in\R$.

This can be used to solve these problems:
\begin{itemize}
  % from Prof Hurtubise's Lecture 14
  \item $y''+y=0$
  \item \textit{(Airy's Equation)} $y''-xy=0$
  \item \textit{(Hermite Equation)} $y''-2xy'+\lambda y=0$
\end{itemize}

Note that this substitution is only valid when the $x_0$ chosen occurs at a
\href{f490530}{regular singular point}.

\Remark{Frobenius series}\label{b1a3270}

This is when we try the substitution
$$
  y:=\sum_{n=0}^\infty a_nx^{r+n}
$$

This is useful for solving these problems:
\begin{itemize}
  \item \textit{(Euler equations)} $x^2y''+x\alpha y'+\beta y=0$. Euler
        equations necessarily have a \href{f490530}{regular singular point}
        (RSP) at $0$.
  \item $2x^2y''-x\alpha y'+(1+x)y=0$. This needs showing that $x=0$ is a
        regular singular point before continuing.
  \item \textit{(Laguerre equation)} $xy''+(1-x)y'+\lambda y=0$. (RSP at 0)
  \item \textit{(Chebyshev equation)} $(1-x^2)y''-xy'+\lambda^2y=0$. (RSP at 1)
        Being the first non-zero RSP, here's how the series expansion looks
        like:
        $$
          y:=\sum_{n=0}^\infty a_n(x-1)^{r+n}
        $$
  \item \textit{(Bessel equation)} $x^2y''+xy'+(x^2-\lambda^2)y=0$. (RSP at 0)
\end{itemize}

\Definition{Dirac delta function}\label{a2fd0bb}

This function is an idealized unit impulse function, denoted by $\delta$. It
has the properties
\begin{gather*}
  \delta(t)=0,\with{t\neq0}\\[0.5em]
  \int_{-\infty}^\infty\delta(t)\,dt=1
\end{gather*}

Since $\delta(t)$ corresponds to a unit impulse at $t=0$, a unit impulse at an
arbitrary point $t_0$ is given by $\delta(t-t_0)$.

% \Theorem{}{}
%
% Consider the differential equation
% \begin{equation*}
%   y''+P(x)y'+Q(x)y=0\Tag{*}
% \end{equation*}
%
% where $x=0$ is a \HREF{f490530}{regular singular point}. Then $xP(x)$ and
% $x^2Q(x)$ are analytic at $x=0$ with convergent power expansions.
%
% \newpage
% \Result{x.x.x}{Exercise}
%
% Consider the equation
% \begin{equation*}
%   L[y]=y''+P(x)y'+Q(x)y=0\Tag{*}
% \end{equation*}
%
% where $xP(x)$ and $x^2Q(x)$ are analytic. That is,
% $$
%   xP(x)=\sum_{n=0}^\infty p_nx^n\quad\text{and}\quad x^2Q(x)=\sum_{n=0}^\infty q_nx^n.
% $$
%
% Assume that a solution for $(*)$ has the form
% $$
%   y=\phi(r,x)=x^r\sum_{n=0}^\infty a_nx^n=\sum_{n=0}^\infty a_nx^{r+n}
% $$
%
% where $a_0\neq0$. It follows that
% \begin{align*}
%   y'  &=\sum_{n=0}^\infty(r+n)a_nx^{r+n-1}        \\
%   y'' &=\sum_{n=0}^\infty(r+n)(r+n-1)a_nx^{r+n-2}
% \end{align*}
%
% Note that
% \begin{align*}
%    &\sum_{n=0}^\infty(r+n)(r+n-1)a_nx^{r+n-2} \\
%    &= r(r-1)a_0x^{r-2}+(r+1)ra_1x^{r-1}
%   +\sum_{n=2}^\infty(r+n)(r+n-1)a_nx^{r+n-2}
% \end{align*}
%
% and
% \begin{align*}
%    &\sum_{n=0}^\infty(r+n)a_nx^{r+n-1}              \\
%    &=ra_0x^{r-1}+\sum_{n=1}^\infty(r+n)a_nx^{r+n-1}
% \end{align*}
%
% Substituting all these power series back into $(*)$, we have
% \begin{align*}
%    &y''+P(x)y'+Q(x)y                                       \\
%    &=\biggl[\sum_{n=0}^\infty(r+n)(r+n-1)a_nx^{r+n-2}\biggr]
%   +P(x)\biggl[\sum_{n=0}^\infty(r+n)a_nx^{r+n-1}\biggr]
%   +Q(x)\biggl[\sum_{n=0}^\infty a_nx^n\biggr]                \\
%    &=\biggl[
%   r(r-1)a_0x^{r-2}+(r+1)ra_1x^{r-1}+P(x)ra_0x^{r-1}
%   \biggr]
%   +\sum_{n=2}^\infty(r+n)(r+n-1)a_nx^{r+n-2}
% \end{align*}
