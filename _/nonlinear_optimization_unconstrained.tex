\chapter{Nonlinear Optimization, Part I: Unconstrained Optimization}\label{fe5d4a2}

\subsection{Basics}\label{afa61c4}

\def\pd{\succ} % positive definite
\def\psd{\succeq} % positive semidefinitenonlu

\Proposition{}\label{bd1eec0}

Let $f:\R^n\to\R$ be continuous. If $\exists c\in\R$ such that $\text{lev}_cf$
is non-empty and bounded then $f$ takes its minimum over $\R^n$.

\Definition{Convex sets ($n$-dim vector space)}\label{e012971}

A set $C\subset\R^n$ is called convex if
$$
  \lambda x+(1-\lambda)y\in C\quad
  \forall(x,y\in C,\ \lambda\in(0,1))
$$

or simply a set which contains all connecting lines of points from the set.

\Definition{Convex functions}\label{a114065}

Let $C\subset\R^n$ be \href{e012971}{convex}. Let $\lambda\in(0,1)$ and $x,y\in
C$ and let

Then $f:C\to\R$ is said to be

\begin{itemize}
  \item convex on $C$ if
        $$
          f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)
        $$
  \item strictly convex on $C$ if
        $$
          f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)
        $$
  \item strongly convex on $C$ if $\exists\mu>0$ such that
        $$
          f(\lambda x+(1-\lambda)y)+\frac{\mu}{2}\lambda(1-\lambda)\norm{x-y}^2\leq\lambda f(x)+(1-\lambda)f(y)
        $$
\end{itemize}

This has parallels \href{cd9cea7}{here} and \href{a8ddd0c}{here}.

\Example{Convex functions}\label{f96c8ae}

\begin{enumerata}
  \item $\exp:\R\to\R$ and $-\log:(0,\infty)\to\R$ are convex.
  \item \textit{(Affine functions)} $f:\R^n\to\R^m$ of the form
  $$
    f(x)=Ax-b\with{(A\in\R^{m\times n},b\in\R^m)}
  $$

  is called \textbf{affine} (linear). All affine functions, hence all linear
  functions ($b=0$) $\R^n\to\R$ are convex.
  \item \textit{(Norms)} Any norm $\norm\cdot$ on $\R^n$ is convex.
\end{enumerata}

\Proposition{Convexity preserving operations}\label{ddce2a7}

\begin{enumerate}
  \item (Positive combinations) For $i=1,\ldots,n$ let $f_i:\R^n\to\R$ be
        convex and $\lambda_i\geq0$. Then $\sum_{i=1}^n\lambda_if_i$ is
        convex.
  \item (Composition with affine mapping) $f:\R^n\to\R$ be convex
        and $g:\R^m\to\R^n$ affine. Then $f\circ G$ is convex.
\end{enumerate}

\Theorem{Taylor's Theorem, $k=2$}\label{b90111f}

Let $D\subset\R^n$ be open, let $f:D\to\R$ be twice continuously
differentiable, and $x,y\in D$ such that $[x,y]\subset D$. Then there exists
$\eta\in[x,y]$ such that
$$
  f(y)-f(x)=\nabla f(x)^T(y-x)+\frac12(y-x)^T\nabla^2f(\eta)(y-x)
$$

\Theorem{Schwarz's Theorem}\label{c3524e9}

Let $D\in\R^n$ be open and $f:D\to\R$ be twice continuously differentiable at
$x\in D$. Then $\nabla^2f(x)$ is symmetric.

\Definition{Directional derivative}\label{b7d1188}

Let $D\subset \R$ be open. $f:D\to\R$ is directionally differentiable at $\bar
x\in\R^n$ in the direction $d\in\R^n\sans0$ if
$$
  \lim_{t\downarrow0}\frac{f(\bar x+td)-f(\bar x)}t
$$

exists. This limit is denoted by $f'(\bar x;d)$ and is called the directional
derivative of $f$ at $\bar x$ in the direction $d$.

If $f$ is directionally differentiable at $\bar x$ in every direction
$d\in\R^n$, we call $f$ directionally differentiable at $\bar x$.

If $f$ is directionally differentiable at every $\bar x\in\R^n$, we call it
directionally differentiable.

\Lemma{Directional derivative and gradient}\label{ed67d29}

Let $D\subset \R^n$ be open and $f:D\to\R$ differentiable at $x\in D$. Then $f$
is directionally differentiable at $x$ with
$$
  f'(x;d)=\nabla f(x)^Td,\with{\forall d\in\R^n}
$$

Where $f'(x;d)$ is the \href{b7d1188}{directional derivative} of $f$ at $x$ in
the direction $d$.

\begin{proof}
  Let $d\in\R^n$ with $\norm d=1$. Then,
  \begin{equation*}
    \frac{f(x+td)-f(x)}{t}-\nabla f(x)^Td
    =\frac{f(x+td)-f(x)-\nabla f(x)^T(td)}{\norm{td}}\Tag{*}
  \end{equation*}

  And then by the \href{c62315d}{differentiability} of $f$ (noting that $\nabla
  f(x)^T$ is the Jacobian of $f$), taking the limit as $t\downarrow0$ on both
  sides, we have that both sides $\to0$.

  It remains to show that this holds for all values of $\norm d$. For all
  $\alpha>0$,
  \begin{align*}
    f'(x;\alpha d)
     &=\lim_{t\downarrow0}\frac{f(x+t\alpha d)-f(x)}t             \\
     &=\lim_{\tau\downarrow0}\frac{f(x+\tau d)-f(x)}{\tau/\alpha} \\
     &=\alpha\lim_{\tau\downarrow0}\frac{f(x+\tau d)-f(x)}\tau    \\
     &=\alpha f'(x;d)
  \end{align*}
\end{proof}

\Lemma{Basic optimality condition}\label{b3b5e10}

Let $X\subset\R^n$ be open and $f:X\to\R$. If $\bar x$ is a local minimizer of
$f$ over $X$ and $f$ is \href{b7d1188}{directionally differentiable} at $\bar
x$ then
$$
  f'(x;d)\geq0,\with{\forall d\in\R^n}
$$

\begin{proof}
  Assume there we $d\in\R^n$ such that $f'(\bar x;d)<0$, i.e.
  $$
    \lim_{t\downarrow0}\frac{f(\bar x+td)-f(\bar x)}t<0
  $$

  In particular, given any $\epsilon>0$, we can always find a $t$ such that
  $$
    f(\bar x+td)<f(\bar x)\Quad\text{and}\Quad \bar x+td\in B_\epsilon(\bar x)
  $$

  This contradicts that $\bar x$ is a \href{bc7900e}{local minimizer} of $f$
  over $X$.
\end{proof}

\Theorem{First-order necessary optimality condition}\label{dc165c9}

Let $X\subset\R^n$ be open and $f:X\to\R$ differentiable at $\bar x\in X$. If
$\bar x$ is a local minimizer (or maximizer) of $f$ over $X$ then $\nabla
f(\bar x)=0$.

\begin{proof}
  By \autoref{b3b5e10}, we have $f'(\bar x;d)\geq0$ for all
  $d\in\R^n$. By \autoref{ed67d29}, this implies
  $$
    \nabla f(\bar x)^Td\geq0,\with{\forall d\in\R^n}
  $$

  Then, by considering different values of $d$, we can easily show that we must
  have $\nabla f(\bar x)=0$.

  For the case of the maximum, consider $-f$ instead.
\end{proof}

\Theorem{Second-order necessary optimality condition}\label{ce5370d}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously differentiable. If
$\bar x$ is a local minimizer (maximizer) of $f$ over $X$ then $\nabla^2f(\bar
x)$ is positive (negative) semidefinite.

\begin{proof}
  \def\x{\bar x}
  \def\H#1{\nabla^2f\ifx&#1&\else(#1)\fi}
  \def\G{\nabla f(\x)}
  We only prove the case in which $\x$ is a local minimizer. The maximum case
  follows from it by substituting $f$ for $-f$.

  Assume on the contrary that $\H\x$ were not positive semidefinite. Then there
  exists $d\in\R^n$ such that $d^T\H\x d<0$. By \href{b90111f}{Taylor's
  Theorem}, for $t>0$ there exists $\eta_t\in[\x,\x+td]$ such that
  $$
    f(\x+td)=f(\x)+t\G^Td+\frac{t^2}2d^T\H{\eta_t}d
  $$

  But since $\x$ is a local minimizer, by \href{dc165c9}{Fermat's rule} we have
  $\G=0$ and hence
  $$
    f(\x+td)=f(\x)+\frac{t^2}2d^T\H{\eta_t}d
  $$

  As $t\downarrow0$, $\eta_t\to\x$ and hence for some $t>0$ sufficiently small,
  we have
  $$
    \frac{t^2}2d^T\H{\eta_t}d<0
  $$

  as $\H{}$ is continuous by assumption. This yields $f(\x+td)<f(\x)$ for all
  $t>0$ sufficiently small, which contradicts the fact that $\x$ is a local
  minimizer of $f$ over $X$. Hence, $\H\x$ must be positive semidefinite.
\end{proof}

\Lemma{}\label{eaa0d24}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously differentiable. If
$\bar x\in\R^n$ is such that $\nabla^2f(\bar x)$ positive definite then
$\exists\mu,\epsilon>0$ such that $B_\epsilon(\bar x)\subset X$ and
$$
  d^T\nabla^2f(x)d\geq\mu\norm d^2_2\quad\forall(d\in\R^n,\
  x\in B_\epsilon(\bar x))
$$

\Theorem{Sufficient optimality condition}\label{b43d95d}

Let $X\subset\R^n$ be open and $f:X\to\R$ twice continuously differentiable.
Moreover, let $\bar x$ be a stationary point of $f$ such that $\nabla^2f(\bar
x)$ is positive definite. Then $\bar x$ is a strict local minimizer of $f$.

\Theorem{First-order characterizations}\label{cd9cea7}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be differentiable on
$C$. Then the following hold for all $x,\bar x\in C$:
\begin{enumerata}
  \item $f$ is convex on $C$ iff
  \begin{equation*}
    f(x)\geq f(\bar x)+\nabla f(\bar x)^T(x-\bar x)\Tag{*}
  \end{equation*}
  \item $f$ is strictly convex on $C$ iff $(*)$ holds with strict
  inequality whenever $x\neq\bar x$.
  \item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff
  $$
    f(x)\geq\Bigl[f(\bar x)+\nabla f(\bar x)^T(x-\bar x)\Bigr]
    +\frac\mu2\norm{x-\bar x}^2
  $$
\end{enumerata}

This has parallels \href{a114065}{here} and \href{a8ddd0c}{here}.

\Corollary{}\label{aa63a8a}

Let $f:\R^n\to\R$ be convex and differentiable. Then the following hold:
\begin{enumerata}
  \item \textit{(Affine minorization)} There exists an
  \href{dcb7f73}{affine} function $g:\R^n\to\R$ which minorizes
  $f$ everywhere, i.e.
  $$
    g(x)\leq f(x)\with{(x\in\R^n)}
  $$
  \item If $f$ is strongly convex then it is strictly convex and coercive
        (level-bounded).
\end{enumerata}

\Corollary{}\label{f2986e2}

Let $f:\R^n\to\R$ be convex and differentiable. Then the following are
equivalent:
\begin{enumerati}
  \item $\bar x$ is a global minimizer of $f$, i.e. $\bar x\in\argmin f$;
  \item $\bar x$ is a local minimizer;
  \item $\bar x$ is a stationary point of $f$.
\end{enumerati}

\begin{proof}
  (i) $\implies$ (ii) is obvious. (ii) $\implies$ (iii) follows from
  \href{dc165c9}{Fermat's Theorem}. (iii) $\implies$ (i) follows
  from \autoref{cd9cea7}.
\end{proof}

\Corollary{Monotonocity of gradient mappings}\label{a8ddd0c}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be differentiable on
$C$. Then the following hold for all $x,y\in C$
\begin{enumerata}
  \item $f$ is convex on $C$ iff
  \begin{equation*}
    \inner{\nabla f(x)-\nabla f(y)}{x-y}\geq0\Tag{*}
  \end{equation*}
  \item $f$ is strictly convex on $C$ iff $(*)$ holds with a strict
  inequality whenever $x\neq y$.
  \item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff
  $$
    \inner{\nabla f(x)-\nabla f(y)}{x-y}\geq\mu\norm{x-y}^2
  $$
\end{enumerata}

This has parallels \href{a114065}{here} and \href{cd9cea7}{here}.

\Theorem{Twice differentiable convex functions}\label{eeb9c30}

Let $C\subset\R^n$ be open and convex and let $f:C\to\R$ be twice continuously
differentiable on $C$. Then the following hold:
\begin{enumerata}
  \def\allx{\forall x\in C}
  \def\hessian{\nabla^2f(x)}
  \item $f$ is convex on $C$ iff $\hessian$ is positive
  semidefinite $\allx$.
  \item If $\hessian$ is positive definite $\allx$ then $f$ is strictly convex
        on $C$.
  \item $f$ is strongly convex with modulus $\mu>0$ on $C$ iff,
  $\allx$, the smallest eigenvalue of $\hessian$ is bounded by $\mu$
  from below.
\end{enumerata}

\Corollary{Convexity of quadratic functions}\label{de25005}

Let $A\in\R^{n\times n}$ be symmetric and $b\in\R^n$, $\gamma\in\R$, and define
$f:\R^n\to\R$ by
$$
  f(x)=\frac12x^TAx+b^Tx+\gamma
$$

Then the following hold:
\begin{enumerata}
  \item $f$ is convex if and only if $A$ is positive semidefinite
  \item $f$ is strongly convex if and only if $A$ is positive definite
\end{enumerata}

\begin{proof}
  % come back here and show that f is twice cont differentiable.
  In view of \autoref{eeb9c30}, it suffices to show that $f$ is twice
  continuously differentiable.
  \begin{align*}
    \nabla f(x)  &= Ax + b \\
    \nabla^2f(x) &= A
  \end{align*}

  and we are done.
\end{proof}

\Theorem{Convex optimization}\label{f546fc9}

Let $f:\R^n\to\R$ be a convex function and $X\subset\R^n$ be a non-empty convex
set. Consider the convex optimization problem
\begin{equation*}
  \begin{array}{l l l}
    \min f(x) & \text{s.t.} & x\in X\Tag{*}
  \end{array}
\end{equation*}

Then the following hold:
\begin{enumerata}
  \item A point $\bar x$ is a global minimizer of $(*)$ if and only if it is a
        local minimizer of $(*)$.
  \item The solution set $\argmin_Xf$ of $(*)$ is convex (possibly empty).
  \item If $f$ is strictly convex, then the solution set has at most one
        element.
  \item If $f$ is strongly convex and differentiable and $X$ is closed, then
        $(*)$ has exactly one solution ($\argmin_Xf$ is a singleton).
\end{enumerata}

\Proposition{Banach Lemma}\label{c733e75}

Let $C\in\R^{n\times n}$ with $\norm C<1$ where $\norm\cdot$ is a
submultiplicative matrix norm. Then $I+C$ is invertible and we have
$$
  \norm{(I+C)^{-1}}\leq\frac1{1-\norm C}
$$

\Corollary{}\label{ae04fae}

Let $A,B\in\R^{n\times n}$ with $\norm{I-BA}<1$ for some submultiplicative norm
$\norm\cdot$. Then $A$ and $B$ are invertible with
$$
  \norm{B^{-1}}\leq\frac{\norm A}{1-\norm{I-BA}}
$$

\Definition{descent direction}\label{ac99a6d}

Let $f:\R^n\to\R$ and $x\in\R^n$. A vector $d\in\R^n$ is said to be a
\textit{descent direction} of $f$ at $x$ if there exists $\ell>0$ such that
$$
  f(x+td)<f(x)\with{(t\in(0,\ell])}
$$

\Proposition{Sufficient condition for descent direction}\label{f81d53c}

Let $f:\R^n\to\R$ be \href{b7d1188}{directionally differentiable} at $x\in\R^n$
in the direction $d\in\R^n$ with
$$
  f'(x;d)<0
$$

Then $d$ is a \href{ac99a6d}{descent direction} of $f$ at $x$. In particular,
this is true if $f$ is differentiable at $x$ with
$$
  \nabla f(x)^Td<0
$$

\begin{proof}
  The first statement follows immediately from the definition of the
  \href{b7d1188}{directional derivative}. The second one uses
  \autoref{ed67d29}.
\end{proof}

\Corollary{}\label{c41d0f0}

Let $f:\R^n\to\R$ be differentiable, $B\in\R^{n\times n}$ be positive definite
and $x\in\R^n$ with $\nabla f(x)\neq0$. Then $-B\nabla f(x)$ is a descent
direction of $f$ at $x$.

\begin{proof}
  This result follows almost immediately from the definition of a
  \href{ac99a6d}{descent direction} and the definition of a
  \href{e25e722}{positive definite} matrix.
\end{proof}

\Definition{Step-size rule}\label{ae4eac6}

Let $f:{\R}^n\to{\R}$ be continuously differentiable and let $\mathcal
A_f:=\{(x,d)\mid\nabla f(x)^Td<0\}$. A set-valued mapping
$$
  T:(x,d)\in\mathcal A_f\mapsto T(x,d)\subset\R_{++}
$$

is called a step-size rule for $f$.

We call it well-defined for $f$ if $T(x,d)\neq0$ for all
$(x,d)\in\mathcal{A}_f$.

If the step-size rule is well-defined for all continuously differentiable
functions ${\R}^n\to{\R}$, we simply call it well-defined.

\Definition{Efficient step-size}\label{d23fdf0}

Let $f:\R^n\to\R$ be continuously differentiable. The \href{ae4eac6}{step-size
rule} $T$ is called efficient for $f$ if there exists $\theta>0$ such that
$$
  f(x+td)\leq f(x)-\theta\left(\frac{\nabla f(x)^Td}{\norm d}\right)^2
$$

\Theorem{Global convergence of general descent method}\label{aa19bbb}

Let $f:\R^n\to\R$ be continuously differentiable, and let $\{x^k\}$, $\{d^k\}$,
$\{t_k\}$ be generated by \autoref{edbf62c}. Moreover, assume that the
following hold:
\begin{enumerati}
  \item \textit{(Angle condition)} There exists $c>0$ such that
  $$
    -\frac{\nabla f(x^k)^Td^k}{\norm{\nabla f(x^k)}\cdot\norm{d^k}}\geq c\with{\forall k\in\N}
  $$

  i.e. the angle between the gradient vector and the descent direction is at
  most $90^\circ$.
  \item \textit{(\href{d23fdf0}{Efficient step-size})} There exists
  $\theta>0$ such that
  $$
    f(x^k+t_kd^k)\leq f(x^k)-\theta\left(
    \frac{\nabla f(x^k)^Td^k}{\norm{d^k}}
    \right)^2\with{\forall k\in\N}
  $$
\end{enumerati}
Then every cluster point of $\{x^k\}$ is a stationary point of $f$.

\begin{proof}
  By (ii), there exists $\theta>0$ such that
  $$
    f(x^{k+1})=f(x^k+t_kd^k)\leq
    f(x^k)-\theta\left(
    \frac{\nabla f(x^k)^Td^k}{\norm{d^k}}
    \right)^2\with{\forall k\in\N}
  $$

  Putting $k:=c^2\theta$, the angle condition implies
  \begin{equation*}
    f(x^{k+1})\leq f(x^k)-k\norm{\nabla f(x^k)}^2\Tag{*}
  \end{equation*}

  Let $\bar x$ be a cluster point of $\{x^k\}$. As $\{f(x^k)\}$ is
  monotonically decreasing and convergent to $f(\bar x)$ on a subsequence
  (since $\{x^k\}\to\bar x$ on a subsequence and $f$ is continuous),
  \href{aaf3ba6}{this implies} that the whole sequence $\{f(x^k)\}$ converges
  to $f(\bar x)$.

  In particular, we have
  $$
    f(x^{k+1})-f(x^k)\to0
  $$

  Therefore, $(*)$ implies $\norm{\nabla f(x^k)}\to0$ by squeezing.
\end{proof}

\Definition{Armijo rule and sufficient decrease}\label{fefb024}

Choose $\beta,\sigma\in(0,1)$. For $x,d\in\href{ae4eac6}{\mathcal A_f}$ the
Armijo rule $T_A$ is defined by
$$
  T_A(x,d)=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq
  f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
$$

The inequality
$$
  f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k
$$

is called the \textit{Armijo condition}. It ensures a \textbf{sufficient
decrease} on the objective function.

\Example{Insufficient decrease}\label{ae7f42d}

Consider the function $f(x)=(x-1)^2-1$ with optimal value $f^*=-1$.

The sequence $\{x_k\}$ with $x_k:=-\frac1k$ has $f(x_k)=\frac{1+2k}{k^2}$ and
$$
  f(x_{k+1})-f(x_k)=\frac{2k^2+4k+1}{k^2(k+1^2)}<0
$$

Hence we've found a case where the objective value decreases, but $f(x_k)$
converges to a non-optimal value. ($f(x_k)\to0$, but we want $f(x_k)\to-1$)

\Lemma{Convergence to gradient}\label{f8e1f12}

Let $f:\R^n\to\R$ be continuously differentiable. Moreover, let
$\{x^k\in\R^n\}\to x$, $\{d^k\in\R^n\}\to d$ and $\{t_k>0\}\downarrow0$. Then
$$
  \lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=\nabla f(x)^Td
$$

\begin{proof}
  By the \href{d37aa2b}{mean value theorem}, for all $k\in\N$, there
  exists $\eta^k\in[x^k,x^k+t_kd^k]$ such that
  $$
    f(x^k+t_kd^k)-f(x^k)=t_k\nabla f(\eta^k)^Td^k
  $$

  Clearly, $\eta^k\to x$ and hence the continuity of $\nabla f$ yields
  $$
    \nabla f(\eta^k)^Td^k\to\nabla f(x)^Td
  $$

  This readily implies
  $$
    \lim_{k\to\infty}\frac{f(x^k+t_kd^k)-f(x^k)}{t_k}=
    \lim_{k\to\infty}\nabla f(\eta^k)^Td^k=
    \nabla f(x)^Td
  $$
\end{proof}

\Theorem{Global convergence of the gradient method}\label{bbb25cd}

Let $f:\R^n\to\R$ be continuously differentiable. Then every cluster point of a
sequence generated by the \href{ae01f6d}{Gradient method with Armijo rule} is a
stationary point of $f$.

\begin{proof}
  \def\xk{\{x^k\}}
  \def\grad#1{\nabla f(#1)}
  Assume on the contrary that $\grad{\bar x}\neq0$.

  Let $\bar x$ be a cluster point of the generated sequence $\xk$, and let
  $\xk_K$ be a subsequence converging to $\bar x$. By the continuity of $f$,
  $\{f(x^k)\}_K\to f(\bar x)$.

  As $\{f(x^k)\}$ is monotonically decreasing by the Armijo condition and
  converges on a subsequence to $f(\bar x)$, \href{aaf3ba6}{by inspection},
  $\{f(x^k)\}_\N$ converges to $f(\bar x)$.

  In particular, we have
  $$
    f(x^k)-f(x^{k+1})\to0
  $$

  Substituting $t_k=\beta^l$ and $x^{k+1}=x^k+\beta^ld^k$ into steps
  \textbf{(S2)} and \textbf{(S3)} of the algorithm, we have
  $$
    0\leq
    t_k\norm{\grad{x^k}}^2=
    -t_k\grad{x^k}^Td^k\leq
    \frac{f(x^k)-f(x^{k+1})}\sigma\to0
  $$

  Since $\{\grad{x^k}\}_K\to\grad{\bar x}\neq0$ (by continuity of $\nabla f$),
  by squeeze theorem on the above inequality, this implies that
  $\{t_k\}_K\to0$. Due to \textbf{(S3)}, for all $k\in K$ sufficiently large,
  we have
  \begin{equation*}
    f(x^k+\beta^{l_k-1}d^k)-f(x^k)>\beta^{l_k-1}\sigma\grad{x^k}^Td^k\Tag{*}
  \end{equation*}

  where $\beta^{l_k}=t_k$ and $l_k\in\N$ is the exponent \textit{uniquely}
  determined by the Armijo rule in \textbf{(S3)}. Note that $l_k$ is the
  smallest value of $l$ that satisfies the \href{fefb024}{Armijo condition},
  and hence $l_k-1$ does \textit{not} satisfy the Armijo condition, hence
  $(*)$.

  Passing to the limit on $K$ and using \autoref{f8e1f12} gives
  $$
    -\norm{\nabla f(\bar x)}^2\geq-\sigma\norm{\nabla f(\bar x)}^2
  $$

  Which is a contradiction because $\sigma\in(0,1)$ and $\nabla f(\bar x)\neq0$
  by assumption. Hence, $\bar x$ is indeed a stationary point of $f$,
  completing the proof.
\end{proof}

\Proposition{Kantorovich inequality}\label{eb4e630}

Let $A\in\R^{n\times n}$ symmetric positive definite. Then

\begin{equation*}
  \def\l{\lambda_{\min}}\def\L{\lambda_{\max}}
  \frac{4\l\L}{(\l+\L)^2}\leq\frac{\norm d^4}{(d^TAd)(d^TA^{-1}d)}
  \with{(\forall d\in\R^n\sans{0})}
\end{equation*}

\Theorem{Gradient method for strongly convex quadratics}\label{a7fa49e}

Let $f:\R^n\to\R$ be given by
$$
  f(x)=\frac12x^TAx+b^Tx
$$

where $A\in\R^{n\times n}$ is symmetric positive definite and $b\in\R^n$. Let
$\bar x:=-A^{-1}b$ be the (unique) global minimizer of $f$. Assume that
$\{x^k\}$ is generated by the gradient method from \autoref{ae01f6d}. Then the
following hold.
\begin{enumerata}
  \item \textit{(Convergence of function values)}
  \begin{equation*}
    \def\l{\lambda_{\min}}\def\L{\lambda_{\max}}
    f(x^{k+1})-f(\bar x)\leq\left(\frac{\L-\l}{\L+\l}\right)^2(f(x^k)-f(\bar x))\with{(\forall k\in\N)}
  \end{equation*}

  i.e. the sequence $\{f(x^k)\}$ converges linearly to $f(\bar x)$.
  \item \textit{(Convergence of variables)}
  \begin{equation*}
    \def\l{\lambda_{\min}}\def\L{\lambda_{\max}}
    \norm{x^k-\bar x}\leq
    \sqrt{\frac\L\l}
    \left(\frac{\L-\l}{\L+\l}\right)^k\norm{x^0-\bar x}\with{\forall k\in\N}
  \end{equation*}

  i.e. $\{x^k\}$ converges to $\bar x$ for any starting point $x^0$.
\end{enumerata}

\Definition{Condition number}\label{bde2dc3}

For a symmetric positive definite matrix $A$, its \textit{condition number} is
given by
$$
  \text{cond}(A):=\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}
$$

The condition number of the matrix influences the convergence rate in
\autoref{a7fa49e}. If $\text{cond}(A)$ is very large then convergence can be
very slow.

\Definition{Convergence rates}\label{b401f98}

Let $\{x^k\in\R^n\}\to\bar x$ and $\norm\cdot$ be an arbitrary norm on $\R^n$.
Then $\{x^k\}$ converges (at least)
\begin{enumerati}
  \def\X{\norm{x^{k+1}-\bar x}}\def\x{\norm{x^{k}-\bar x}}
  \item \textbf{linearly} to $\bar x$ if there exists $c\in(0,1)$ such that
  $$
    \X\leq c\x\with{(\forall k\in\N\text{ sufficiently large})}
  $$
  \item \textbf{superlinearly} to $\bar x$ if
  $$
    \lim_{k\to\infty}\frac\X\x=0
  $$

  or equivalently, if there is a sequence of constants $\{c_k\}\to0$ such that
  $$
    \X\leq c_k\x\with{(\forall k\in\N\text{ sufficiently large})}
  $$
  \item \textbf{quadratically} to $\bar x$ if there exists $C>0$ such that
  $$
    \X\leq C\x^2\with{(\forall k\in\N\text{ sufficiently large})}
  $$
\end{enumerati}

\Remark{Convergence rates with Landau notation}\label{e32ba11}

Rewriting \autoref{b401f98} using \href{ab54b3a}{Landau notation}, we say
$\{x^k\}\to\bar x$ converges
\begin{enumerati}
  \item superlinearly if and only if
  $$
    \norm{x^{k+1}-\bar x}=o(\norm{x^{k}-\bar x})
  $$
  \item quadratically if and only if
  $$
    \norm{x^{k+1}-\bar x}=O(\norm{x^{k}-\bar x}^2)
  $$
\end{enumerati}

\Remark{Newton's method}\label{d6c0554}

Our goal is to effectively solve
$$
  F(x)=0
$$

where $F:\R^n\to\R^n$ is assumed to be continuously differentiable. The method
we are going to study is called \textit{Newton's method} and its basic idea is
shockingly simple and relies on \textit{linearization}, one of the most basic
principles in mathematics:

Suppose $\bar x$ is a root of $F$ and $x^k$ is our current approximation of it.
Then consider a local, linear approximation
$$
  x\mapsto F_k(x):=F(x^k)+F'(x^k)(x-x^k)
$$

of $F$ at $x^k$. Now, compute $x^{k+1}$ as a root of $F_k$, and we should move
closer to $\bar x$.

If $F'(x^k)\in\R^{n\times n}$ is invertible we can write
$$
  x^{k+1}=x^k-F'(x^k)^{-1}F(x^k)
$$

But for numerical reasons one does not explicitly invert a matrix, but instead
will compute the \textit{Newton direction} $d^k$ as solution to the
\textit{Newton equation}
$$
  F'(x^k)d=-F(x^k)
$$

and then update $x^{k+1}:=x^k+d^k$. This yields \autoref{abbc9be}.

\Lemma{Local invertibility}\label{ad66265}

Let $F:\R^n\to\R^n$ be continuously differentiable and $\bar x\in\R^n$ such
that $F'(\bar x)$ is invertible. Then there exists $\epsilon>0$ such that
$F'(x)$ is invertible for all $x\in B_\epsilon(\bar x)$. Moreover, there exists
$c>0$ such that
$$
  \norm{F'(x)^{-1}}\leq c\with{(x\in B_\epsilon(\bar x))}
$$

% #[comment]
% If F is cont. differentiable and F'(x̄) is invertible, then F' is
% invertible in a neighborhood. Moreover, the norm of inverse F' is
% bounded in that neighborhood.

\Remark{Differentiability in Landau notation}\label{b4ee31d}

Using the \href{ab54b3a}{Landau notation}, we can express the fact that $F$ is
\href{c62315d}{differentiable} at $\bar x\in\R^n$ if and only if
$$
  \norm{F(x^k)-F(\bar x)-F'(x^k)(x^k-\bar x)}=o(\norm{x^{k}-\bar x})
$$

for all sequences $\{x^k\}\to\bar x$.

Here's how it's expanded:
$$
  \lim_{x^k\to\bar x}\frac{\norm{F(x^k)-F(\bar x)-F'(x^k)(x^k-\bar x)}}{\norm{x^k-\bar x}}=0
$$

\Definition{Local Lipschitz}\label{ba65fa0}

We say that $G:\R^n\to\R^m$ is locally Lipschitz (continuous) at $\bar
x\in\R^n$ if there exists $L=L(\bar x)>0$ such that
$$
  \norm{G(x)-G(y)}\leq L\norm{x-y}\with{(x,y\in B_\epsilon(\bar x))}
$$

\Lemma{}\label{a71a60e}

Let $F:\R^n\to\R^n$ be continuously differentiable and $\{x^k\}$ such that
$\{x^k\}\to\bar x$. Then
\begin{equation*}
  \norm{F(x^k)-F(\bar x)-F'(x^k)(x^k-\bar x)}=o(\norm{x^{k}-\bar x})
\end{equation*}

If $F'$ is, in addition, \href{ba65fa0}{locally Lipschitz continuous}, we also
have
\begin{equation*}
  \norm{F(x^k)-F(\bar x)-F'(x^k)(x^k-\bar x)}=O(\norm{x^{k}-\bar x}^2)
\end{equation*}

\Theorem{Convergence of local Newton's method for equations}\label{fc03f3f}

Let $F:\R^n\to\R^n$ be continuously differentiable and let $\bar x$ be a root
of $F$ such that $F'(\bar x)$ is invertible. Then there exists $\epsilon>0$
such that for every $x^0\in B_\epsilon(\bar x)$, the following hold:
\begin{enumerata}
  \item The local Newton method from \autoref{abbc9be} is well-defined and
        generates a sequence $\{x^k\}$ convergent to $\bar x$.
  \item The rate of convergence is at least superlinear.
  \item If in addition $F'$ is \href{ba65fa0}{locally Lipschitz continuous} at
        $\bar x$ the rate of convergence is quadratic.
\end{enumerata}

\Remark{Newton's method in optimization}\label{f2a142c}

We now want to exploit our study of Newton's method for solving smooth,
nonlinear equations to tackle unconstrained optimization problems of the form
$$
  \min_{x\in\R^n} f(x)
$$

where $f:\R^n\to\R$ is at least twice continuously differentiable. Recall that
a necessary condition for $\bar x$ to be a local minimizer of $f$ is
$$
  \nabla f(\bar x)=0
$$

So we can put $F:=\nabla f$ and we have $F:\R^n\to\R$ continuously
differentiable, and all local minimizers of $f$ are among its roots.

This yields \autoref{c2bcc4e}.

\Theorem{Convergence of local Newton's method for optimization}\label{bf187b2}

Let $f:\R^n\to\R$ be twice continuously differentiable and let $\bar x$ be a
stationary point of $f$ such that $\nabla^2f(\bar x)$ is invertible. Then there
exists $\epsilon>0$ such that for every $x^0\in B_\epsilon(\bar x)$, the
following hold:
\begin{enumerata}
  \item \autoref{c2bcc4e} is well-defined and generates
  a sequence $\{x^k\}$ convergent to $\bar x$.
  \item The rate of convergence is at least superlinear.
  \item If in addition $\nabla^2f$ is \href{ba65fa0}{locally Lipschitz
        continuous} at $\bar x$ the rate of convergence is quadratic.
\end{enumerata}

\Theorem{Global convergence of \autoref{a7a5665}}\label{a66db73}

Let $f:\R^n\to\R$ be twice continuously differentiable. Then every cluster
point of a sequence generated by \autoref{a7a5665} is a stationary point of
$f$.

% \begin{proof}
% 	\def\xk{\{x^k\}}
% 	\def\grad{\nabla f(x^k)}
%
% 	Let $\xk$ be generated by \AUTOREF{a7a5665}{}. Let
% 	$\bar x$ be a cluster point with $\xk_K\to\bar x$. (This exists
% 	because $\xk$ is monotone decreasing and hence there exists a
% 	subsequence $\xk_K$ that converges to any given cluster point)
%
% 	If $d^k=\grad$ for infinitely $k\in K$ then the assertion follows immediately from
% \end{proof}

\Lemma{Moré and Sorensen}\label{a95dfe1}

Let $\bar x$ be an isolated cluster point of $\{x^k\in\R^n\}$ and assume that
$\{\norm{x^{k+1}-x^k}\}_K\to0$ for every subsequence $\{x^k\}_K\to\bar x$. Then
the whole sequence $\{x^k\}$ converges to $\bar x$.

\Corollary{}\label{aabc89a}

Let $\bar x$ be an isolated cluster point of a sequence $\{x^k\}$ generated by
\autoref{a7a5665}. Then the whole sequence $\{x^k\}$ converges to $\bar x$.

\Proposition{Acceptance of full step-size in globalized Newton's method}\label{d6d6148}

Let $f:\R^n\to\R$ be twice continuously differentiable, and $\bar x\in\R^n$
such that $\nabla f(\bar x)=0$ and $\nabla^2f(\bar x)$ is positive definite.
Assume that $\{x^k\}\to\bar x$ and that $d^k$ is given by
\begin{gather*}
  d^k = -\nabla^2f(x^k)^{-1}\nabla f(x^k) \\
  \text{\footnotesize{(\href{d6c0554}{Newton Equation})}}
\end{gather*}

Then there exists $k_0\in\N$ such that $\forall\sigma\in(0,\frac12),\ k\geq
k_0$:
$$
  f(x^k+d^k)\leq f(x^k)+\sigma\nabla f(x^k)^Td^k
$$

The significance of this last equation being that $f$ experiences
\href{fefb024}{sufficient decrease}.

\Theorem{}\label{e58830c}

Let $f:\R^n\to\R$ be twice continuously differentiable and let $\{x^k\}$ be
generated by \autoref{a7a5665}. If $\bar x$ is a cluster point of $\{x^k\}$
such that $\nabla^2f(\bar x)$ is positive definite then the following hold:
\begin{enumerata}
  \item The whole sequence $\{x^k\}$ converges to $\bar x$ and $\bar x$ is a
        strict local minimizer of $f$.
  \item For all $k\in\N$ sufficiently large, the search direction $d^k$ will be
        determined through the \href{d6c0554}{Newton equation}.
  \item For all $k\in\N$ sufficiently large, the full step-size $t_k=1$ will be
        accepted
  \item $\{x^k\}$ converges superlinearly to $\bar x$
  \item If $\nabla^2f$ is locally Lipschitz then $\{x^k\}$ converges to $\bar
        x$ quadratically.
\end{enumerata}

\Definition{Quasi-Newton equation}\label{cc538a4}

In the context of iterating over $\{x_k\}$ and $\{H_k\}$ where $H_k$ is the
approximation of $\nabla^2f(x^k)$ at $x^k$, $H_{k+1}$ satisfies the
quasi-Newton equation if
$$
  H_{k+1}s^k=y^k,
$$

where $s^k:=x^{k+1}-x^k$ and $y^k:=\nabla f(x^{k+1})-\nabla f(x^k)$.

\Remark{Direct Quasi-Newton methods}\label{f25ca2a}

In order to devise a strategy of how to approximate the Hessian matrix of the
underlying function $f$ the current iterate $x^k$ we first need to agree on
which properties we would like it to have. To this end, let $H_k$ be an
approximation of $\nabla^2f(x^k)$. We would like for $H_{k+1}$ to satisfy the
following criteria:
\begin{enumerate}
  \item [I.] $H_{k+1}=H_{k+1}^T$ is symmetric.
  \item [II.] $H_{k+1}$ satisfies the \href{cc538a4}{quasi-Newton equation}.
  \item [III.] $H_{k+1}$ can be obtained efficiently from $H_k$.
  \item [IV.] The resulting method has strong local convergence properties.
\end{enumerate}

\Remark{}\label{b19fe28}

Let $x^k$ be a current iterate for minimizing $f:\R^n\to\R$ that is twice
continuously differentiable.
\begin{enumerata}
  \item The Hessian $\nabla^2f(x^k)$ of $f$ at $x^k$ does not necessarily
        satisfy the \href{cc538a4}{quasi-Newton equation}.
  \item Condition I. is motivated by \href{c3524e9}{Schwarz's Theorem}.
  \item The quasi-Newton equation can be motivated by the Mean-Value Theorem in
        integral form, which yields
  \begin{align*}
    \nabla f(x^{k+1})-\nabla f(x^k)
        &=\int_0^1\nabla^2f(x^k+t(x^{k+1}-x^k))\,dt\cdot(x^{k+1}-x^k) \\
    y^k &=\int_0^1\nabla^2f(x^k+t(x^{k+1}-x^k))\,dt\cdot s^k
  \end{align*}

  The term $\int_0^1\nabla^2f(x^k+t(x^{k+1}-x^k))\,dt$ can be interpreted as
  some sort of ``averaged Hessian".
\end{enumerata}

\Remark{Collection of unconstrained minimization methods}\label{a75d03d}

\begin{enumerate}
  \item Gradient method
  \item Globalized Newton's method
  \item Globalized BFGS method
  \item Globalized inexact Newton's method
\end{enumerate}

\Algorithm{General line-search descent algorithm}\label{edbf62c}

Goal is to solve
$$
  \min_{x\in\R^n}f(x).
$$

\begin{enumerate}
  \item [\textbf{(S0)}] \textit{Initialization}: Choose $x^0\in\R^n$ and put $k:=0$.
  \item [\textbf{(S1)}] \textit{Termination}: If $x^k$ satisfies a termination criterion: STOP.
  \item [\textbf{(S2)}] \textit{Search direction}: Determine $d^k$ such that $\nabla f(x^k)^Td^k<0$.
  \item [\textbf{(S3)}] \textit{Step-size}: Determine $t_k>0$ such that $f(x^k+t_kd^k)<f(x^k)$.
  \item [\textbf{(S4)}] \textit{Update}: Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{Gradient method with Armijo rule}\label{ae01f6d}

Goal is to solve
$$
  \min_{x\in\R^n}f(x).
$$

\begin{enumerate}
  \item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\sigma,\beta\in(0,1)$, $\epsilon\geq0$ and put $k:=0$.
  \item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
  \item [\textbf{(S2)}] Put $d^k:=-\nabla f(x^k)$.
  \item [\textbf{(S3)}] Determine $t_k>0$ by
        $$
          t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
        $$
  \item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{Local Newton's method for equations}\label{abbc9be}

Goal is to solve
$$
  F(x)=0
$$

where $F:\R^n\to\R^n$ and $F$ is assumed to be continuously differentiable.

\begin{enumerate}
  \item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
  \item [\textbf{(S1)}] If $\norm{F(x^k)}\leq\epsilon$, STOP.
  \item [\textbf{(S2)}] Compute $d^k$ as a solution of
        $$
          F'(x^k)d=-F(x^k)
        $$
  \item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{Local Newton's method for unconstrained optimization}\label{c2bcc4e}

Goal is to solve
$$
  \min_{x\in\R^n}f(x).
$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\href{abbc9be}{local Newton's method for equations}.

\begin{enumerate}
  \item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\epsilon\geq0$ and put $k:=0$.
  \item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
  \item [\textbf{(S2)}] Compute $d^k$ as a solution of
        $$
          \nabla^2f(x^k)d=-\nabla f(x^k)
        $$
  \item [\textbf{(S3)}] Put $x^{k+1}:=x^k+d^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}

\Algorithm{Globalized Newton's method for unconstrained optimization}\label{a7a5665}

Goal is to solve
$$
  \min_{x\in\R^n}f(x).
$$

The following is obtained by substituting $F$ for $\nabla f$ in the
\href{abbc9be}{local Newton's method for equations}.

\begin{enumerate}
  \item [\textbf{(S0)}] Choose $x^0\in\R^n$, $\rho>0$, $p>2$, $\beta\in(0,1)$, $\sigma\in(0,\frac12)$, $\epsilon\geq0$ and put $k:=0$.
  \item [\textbf{(S1)}] If $\norm{\nabla f(x^k)}\leq\epsilon$, STOP.
  \item [\textbf{(S2)}] Try to compute $d^k$ as a solution of
        $$
          \nabla^2f(x^k)d=-\nabla f(x^k)
        $$

        If no solution can be found or if
        \begin{gather*}
          \nabla f(x^k)^Td^k>-\rho\norm{d^k}^p\\
          \text{\footnotesize{(\href{fefb024}{insufficient decrease})}}
        \end{gather*}

        then fall back to $d^k:=-\nabla f(x^k)$
  \item [\textbf{(S3)}] Determine $t_k$ by
        $$
          t_k:=\max_{l\in\N_0}\{\beta^l\mid f(x^k+\beta^ld^k)\leq f(x^k)+\beta^l\sigma\nabla f(x^k)^Td^k\}
        $$
  \item [\textbf{(S4)}] Put $x^{k+1}:=x^k+t_kd^k$, $k\gets k+1$ and go to \textbf{(S1)}.
\end{enumerate}
